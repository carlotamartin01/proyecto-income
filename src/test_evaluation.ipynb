{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0c04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217398f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"../data/processed/X_test.csv\")\n",
    "y_test = pd.read_csv(\"../data/processed/y_test.csv\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46af7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# cargo los modelos #\n",
    "#####################\n",
    "\n",
    "# En principio solo iba a poner el mejor que es el LightGBM pero finalmente lo hare de los dos\n",
    "\n",
    "with open(\"../data/models/modelo_lgbm.pkl\", \"rb\") as f:\n",
    "    modelo_lgbm = pickle.load(f)\n",
    "\n",
    "with open(\"../data/models/modelo_rf.pkl\", \"rb\") as f:\n",
    "    modelo_rf = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5daeab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Funcion de evaluacion #\n",
    "#########################\n",
    "\n",
    "def evaluar_modelo(modelo, X, y, nombre):\n",
    "    pred = modelo.predict(X)\n",
    "    prob = modelo.predict_proba(X)[:, 1]\n",
    "\n",
    "    print(f\"\\nResultados en TEST {nombre}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y, pred))\n",
    "    print(\"Precision:\", precision_score(y, pred))\n",
    "    print(\"Recall:\", recall_score(y, pred))\n",
    "    print(\"F1 Score:\", f1_score(y, pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y, prob))\n",
    "    print(\"Matriz de confusión:\\n\", confusion_matrix(y, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7420026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados en TEST LightGBM\n",
      "Accuracy: 0.8712850994056159\n",
      "Precision: 0.7757916241062308\n",
      "Recall: 0.6502568493150684\n",
      "F1 Score: 0.7074988355845365\n",
      "ROC AUC: 0.9243762700579174\n",
      "Matriz de confusión:\n",
      " [[6983  439]\n",
      " [ 817 1519]]\n",
      "\n",
      "Resultados en TEST Random Forest\n",
      "Accuracy: 0.8634966181594589\n",
      "Precision: 0.7865296803652968\n",
      "Recall: 0.5898972602739726\n",
      "F1 Score: 0.6741682974559687\n",
      "ROC AUC: 0.9151570453723288\n",
      "Matriz de confusión:\n",
      " [[7048  374]\n",
      " [ 958 1378]]\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Evaluación de los modelos #\n",
    "#############################\n",
    "\n",
    "evaluar_modelo(modelo_lgbm, X_test, y_test, \"LightGBM\")\n",
    "evaluar_modelo(modelo_rf, X_test, y_test, \"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89466abf",
   "metadata": {},
   "source": [
    "He probado los dos modelos en test y ambos se han comportado de forma coherente con la validación. El modelo LightGBM ha conseguido el mejor equilibrio entre todas las métricas, especialmente en F1 Score y ROC AUC, por lo que lo selecciono como modelo final para producción.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
